{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e8f489",
   "metadata": {},
   "source": [
    "# Voice Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0701129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funasr version: 1.2.6.\n",
      "Check update of funasr, and it would cost few times. You may disable it by set `disable_update=True` in AutoModel\n",
      "You are using the latest version of funasr-1.2.6\n",
      "Loading remote code successfully: ./model.py\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not found: \"C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\SenseVoiceSmall\\chn_jpn_yue_eng_ko_spectok.bpe.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型路径不存在: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 加载模型（使用本地路径）\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 直接传入本地绝对路径\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 确保model.py在当前工作目录\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvad_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfsmn-vad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvad_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_single_segment_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 后续识别和解析代码保持不变\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecognize_cantonese_audio\u001b[39m(audio_path):\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\funasr\\auto\\auto_model.py:125\u001b[0m, in \u001b[0;36mAutoModel.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m log_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logging, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_level\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mupper())\n\u001b[0;32m    123\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlog_level)\n\u001b[1;32m--> 125\u001b[0m model, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# if vad_model is not None, build vad model else None\u001b[39;00m\n\u001b[0;32m    128\u001b[0m vad_model \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvad_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\funasr\\auto\\auto_model.py:225\u001b[0m, in \u001b[0;36mAutoModel.build_model\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     tokenizer_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseg_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seg_dicts[i]\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m### === only for kws ===\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizer_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_conf)\n\u001b[0;32m    226\u001b[0m tokenizers_build\u001b[38;5;241m.\u001b[39mappend(tokenizer)\n\u001b[0;32m    227\u001b[0m token_list \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtoken_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_list\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\funasr\\tokenizer\\sentencepiece_tokenizer.py:23\u001b[0m, in \u001b[0;36mSentencepiecesTokenizer.__init__\u001b[1;34m(self, bpemodel, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# NOTE(kamo):\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Don't build SentencePieceProcessor in __init__()\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# because it's not picklable and it may cause following error,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# \"TypeError: can't pickle SwigPyObject objects\",\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# when giving it as argument of \"multiprocessing.Process()\".\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_sentence_piece_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\funasr\\tokenizer\\sentencepiece_tokenizer.py:32\u001b[0m, in \u001b[0;36mSentencepiecesTokenizer._build_sentence_piece_processor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbpemodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\DL\\lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\SenseVoiceSmall\\chn_jpn_yue_eng_ko_spectok.bpe.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from funasr import AutoModel\n",
    "from funasr.utils.postprocess_utils import rich_transcription_postprocess\n",
    "\n",
    "# 本地模型路径（替换为你的实际路径）\n",
    "model_dir = r\"C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\SenseVoiceSmall\"\n",
    "\n",
    "# 验证路径是否存在\n",
    "if not os.path.exists(model_dir):\n",
    "    raise FileNotFoundError(f\"模型路径不存在: {model_dir}\")\n",
    "\n",
    "# 加载模型（使用本地路径）\n",
    "model = AutoModel(\n",
    "    model=model_dir,  # 直接传入本地绝对路径\n",
    "    trust_remote_code=True,\n",
    "    remote_code=\"./model.py\",  # 确保model.py在当前工作目录\n",
    "    vad_model=\"fsmn-vad\",\n",
    "    vad_kwargs={\"max_single_segment_time\": 30000},\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# 后续识别和解析代码保持不变\n",
    "def recognize_cantonese_audio(audio_path):\n",
    "    res = model.generate(\n",
    "        input=audio_path,\n",
    "        cache={},\n",
    "        language=\"yue\",  # 指定粤语\n",
    "        use_itn=True,\n",
    "        batch_size_s=60,\n",
    "        merge_vad=True,\n",
    "        merge_length_s=15\n",
    "    )\n",
    "    return rich_transcription_postprocess(res[0][\"text\"])\n",
    "\n",
    "# 固定指令集\n",
    "command_set = [\"开窗\", \"关窗\", \"开灯\", \"关灯\"]\n",
    "\n",
    "# 音频路径（替换为你的文件）\n",
    "audio_path = \"audio/开灯.m4a\"\n",
    "\n",
    "# 执行识别和解析\n",
    "if os.path.exists(audio_path):\n",
    "    recognized_text = recognize_cantonese_audio(audio_path)\n",
    "    print(f\"识别结果: {recognized_text}\")\n",
    "    \n",
    "    matched_command = next((cmd for cmd in command_set if cmd in recognized_text), \"\")\n",
    "    print(f\"匹配指令: {matched_command if matched_command else '未匹配'}\")\n",
    "else:\n",
    "    print(f\"音频文件不存在: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8483fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建临时目录: D:\\TEMPFI~1\\tmpupayp77a\n",
      "正在复制模型文件到临时目录...\n",
      "加载模型中...\n",
      "funasr version: 1.2.6.\n",
      "Loading remote code successfully: model\n",
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\speech_fsmn_vad_zh-cn-16k-common-pytorch\n",
      "开始处理音频: audio/开灯_test.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.163: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00,  1.52it/s]                                                                                          \n",
      "rtf_avg: 0.190: 100%|\u001b[34m██████████\u001b[0m| 1/1 [00:00<00:00,  3.74it/s]\n",
      "rtf_avg: 0.071, time_speech:  4.032, time_escape: 0.285: 100%|\u001b[31m██████████\u001b[0m| 1/1 [00:00<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别结果: 开灯。😔\n",
      "匹配命令: 开灯\n",
      "清理临时文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import tempfile\n",
    "from funasr import AutoModel\n",
    "from funasr.utils.postprocess_utils import rich_transcription_postprocess\n",
    "\n",
    "# 创建临时目录（英文路径）\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"创建临时目录: {temp_dir}\")\n",
    "\n",
    "# 原始模型目录（含中文路径）\n",
    "source_dir = r\"C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\SenseVoiceSmall\"\n",
    "\n",
    "# 检查模型目录是否存在\n",
    "if not os.path.exists(source_dir):\n",
    "    raise FileNotFoundError(f\"模型路径不存在: {source_dir}\")\n",
    "\n",
    "# 将模型复制到临时目录（纯英文路径）\n",
    "print(\"正在复制模型文件到临时目录...\")\n",
    "model_temp_dir = os.path.join(temp_dir, \"SenseVoiceTemp\")\n",
    "shutil.copytree(source_dir, model_temp_dir, dirs_exist_ok=True)\n",
    "\n",
    "# 预加载VAD模型避免下载\n",
    "vad_dir = os.path.join(temp_dir, \"speech_fsmn_vad_zh-cn-16k-common-pytorch\")\n",
    "os.makedirs(vad_dir, exist_ok=True)\n",
    "# 从本地缓存复制VAD模型文件\n",
    "vad_source_dir = r\"C:\\Users\\DJH的Y9000K\\.cache\\modelscope\\hub\\models\\iic\\speech_fsmn_vad_zh-cn-16k-common-pytorch\"\n",
    "if os.path.exists(vad_source_dir):\n",
    "    shutil.copytree(vad_source_dir, vad_dir, dirs_exist_ok=True)\n",
    "else:\n",
    "    print(\"警告：未找到本地VAD模型缓存，将从网络下载\")\n",
    "\n",
    "# 加载模型\n",
    "print(\"加载模型中...\")\n",
    "model = AutoModel(\n",
    "    model=model_temp_dir,  # 使用英文路径\n",
    "    trust_remote_code=True,\n",
    "    vad_model=\"fsmn-vad\",\n",
    "    vad_model_path=vad_dir,  # 指定本地VAD模型路径\n",
    "    vad_kwargs={\"max_single_segment_time\": 30000},\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    disable_update=True  # 禁止检查更新\n",
    ")\n",
    "\n",
    "def recognize_cantonese_audio(audio_path):\n",
    "    \"\"\"处理音频识别的函数\"\"\"\n",
    "    try:\n",
    "        res = model.generate(\n",
    "            input=audio_path,\n",
    "            cache={},\n",
    "            language=\"yue\",  # 指定粤语\n",
    "            use_itn=True,\n",
    "            batch_size_s=60,\n",
    "            merge_vad=True,\n",
    "            merge_length_s=15\n",
    "        )\n",
    "        return rich_transcription_postprocess(res[0][\"text\"])\n",
    "    except Exception as e:\n",
    "        print(f\"语音识别失败: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 固定指令集\n",
    "command_set = [\"开窗\", \"关窗\", \"开灯\", \"关灯\"]\n",
    "\n",
    "# 已经转换好的WAV文件路径\n",
    "wav_path = \"audio/开灯_test.wav\"  # 根据实际文件名修改\n",
    "\n",
    "# 确保WAV文件存在\n",
    "if os.path.exists(wav_path):\n",
    "    print(f\"开始处理音频: {wav_path}\")\n",
    "    \n",
    "    # 语音识别\n",
    "    recognized_text = recognize_cantonese_audio(wav_path)\n",
    "    print(f\"识别结果: {recognized_text}\")\n",
    "    \n",
    "    # 命令匹配\n",
    "    matched_command = \"\"\n",
    "    max_common_characters = 0\n",
    "    \n",
    "    for cmd in command_set:\n",
    "        # 计算共有字符数量\n",
    "        common_count = sum(1 for char in cmd if char in recognized_text)\n",
    "        \n",
    "        # 如果完全包含某个命令\n",
    "        if cmd in recognized_text:\n",
    "            matched_command = cmd\n",
    "            break\n",
    "        # 否则选择共有字符最多的命令\n",
    "        elif common_count > max_common_characters:\n",
    "            matched_command = cmd\n",
    "            max_common_characters = common_count\n",
    "            \n",
    "    if matched_command:\n",
    "        print(f\"匹配命令: {matched_command}\")\n",
    "    else:\n",
    "        print(\"未匹配到有效指令\")\n",
    "else:\n",
    "    print(f\"音频文件不存在: {wav_path}\")\n",
    "\n",
    "# 清理临时目录\n",
    "print(\"清理临时文件...\")\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1466eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
